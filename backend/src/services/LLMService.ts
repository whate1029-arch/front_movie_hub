import OpenAI from 'openai';
import { Movie, LLMMovieCard } from '../models/Movie';
import { Logger } from '../utils/Logger';
import { ApiUsageLogger } from '../utils/ApiUsageLogger';
import NodeCache from 'node-cache';

export interface LLMConfig {
  apiKey: string;
  model: string;
  maxTokens: number;
  temperature: number;
  timeout: number;
}

export interface MovieCardGenerationOptions {
  includeActorImages?: boolean;
  summaryLength?: 'short' | 'medium' | 'long';
  tone?: 'professional' | 'casual' | 'enthusiastic';
  targetAudience?: 'general' | 'critics' | 'families';
}

export class LLMService {
  private openai: OpenAI;
  private config: LLMConfig;
  private cache: NodeCache;
  private apiUsageLogger: ApiUsageLogger;
  private logger: Logger;

  constructor(config: LLMConfig, apiUsageLogger: ApiUsageLogger) {
    this.config = config;
    this.openai = new OpenAI({
      apiKey: config.apiKey,
      timeout: config.timeout,
    });
    
    // Cache LLM responses for 24 hours to reduce API costs
    this.cache = new NodeCache({ 
      stdTTL: 24 * 60 * 60, // 24 hours
      checkperiod: 60 * 60, // Check for expired keys every hour
      useClones: false
    });
    
    this.apiUsageLogger = apiUsageLogger;
    this.logger = new Logger();
  }

  /**
   * Generate a structured movie card using LLM analysis
   */
  async generateMovieCard(
    movie: Movie, 
    options: MovieCardGenerationOptions = {}
  ): Promise<LLMMovieCard> {
    const startTime = Date.now();
    const cacheKey = this.generateCacheKey(movie, options);
    
    try {
      // Check cache first
      const cachedResult = this.cache.get<LLMMovieCard>(cacheKey);
      if (cachedResult) {
        this.logger.info('LLM movie card retrieved from cache', { 
          movieId: movie.imdbId,
          title: movie.title 
        });
        return cachedResult;
      }

      // Generate prompt for LLM
      const prompt = this.buildMovieCardPrompt(movie, options);
      
      this.logger.info('Generating movie card with LLM', { 
        movieId: movie.imdbId,
        title: movie.title,
        model: this.config.model
      });

      // Call OpenAI API
      const response = await this.openai.chat.completions.create({
        model: this.config.model,
        messages: [
          {
            role: 'system',
            content: this.getSystemPrompt()
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: this.config.maxTokens,
        temperature: this.config.temperature,
        response_format: { type: 'json_object' }
      });

      const responseTime = Date.now() - startTime;

      // Log API usage
      this.apiUsageLogger.logRequest({
        apiSource: 'openai',
        endpoint: '/chat/completions',
        responseStatus: 200,
        responseTimeMs: responseTime
      });

      // Parse and validate response
      const generatedContent = response.choices[0]?.message?.content;
      if (!generatedContent) {
        throw new Error('No content generated by LLM');
      }

      const movieCard = this.parseAndValidateResponse(generatedContent, movie);
      
      // Cache the result
      this.cache.set(cacheKey, movieCard);
      
      this.logger.info('Movie card generated successfully', { 
        movieId: movie.imdbId,
        responseTime,
        tokensUsed: response.usage?.total_tokens
      });

      return movieCard;

    } catch (error) {
      const responseTime = Date.now() - startTime;
      
      this.apiUsageLogger.logRequest({
        apiSource: 'openai',
        endpoint: '/chat/completions',
        responseStatus: error instanceof Error ? 500 : 400,
        responseTimeMs: responseTime,
        errorMessage: error instanceof Error ? error.message : 'Unknown error'
      });

      this.logger.error('Failed to generate movie card', {
        movieId: movie.imdbId,
        error: error instanceof Error ? error.message : 'Unknown error',
        responseTime
      });

      throw new Error(`Failed to generate movie card: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Generate multiple movie cards in batch
   */
  async generateMovieCardsBatch(
    movies: Movie[], 
    options: MovieCardGenerationOptions = {}
  ): Promise<LLMMovieCard[]> {
    const results: LLMMovieCard[] = [];
    const batchSize = 5; // Process 5 movies at a time to avoid rate limits
    
    for (let i = 0; i < movies.length; i += batchSize) {
      const batch = movies.slice(i, i + batchSize);
      const batchPromises = batch.map(movie => 
        this.generateMovieCard(movie, options).catch(error => {
          this.logger.error('Failed to generate movie card in batch', {
            movieId: movie.imdbId,
            error: error.message
          });
          return null;
        })
      );
      
      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults.filter(result => result !== null) as LLMMovieCard[]);
      
      // Add delay between batches to respect rate limits
      if (i + batchSize < movies.length) {
        await this.delay(1000); // 1 second delay
      }
    }
    
    return results;
  }

  /**
   * Regenerate movie card with different options
   */
  async regenerateMovieCard(
    movie: Movie, 
    options: MovieCardGenerationOptions = {}
  ): Promise<LLMMovieCard> {
    // Clear cache for this movie
    const cacheKey = this.generateCacheKey(movie, options);
    this.cache.del(cacheKey);
    
    return this.generateMovieCard(movie, options);
  }

  /**
   * Get system prompt for LLM
   */
  private getSystemPrompt(): string {
    return `You are an expert movie analyst and content creator. Your task is to generate original, engaging movie cards based on provided movie data.

IMPORTANT GUIDELINES:
1. Generate ORIGINAL content - never copy existing plot summaries or reviews
2. Create fresh, unique descriptions that capture the essence of the movie
3. Ensure all content is copyright-compliant and transformative
4. Focus on themes, emotions, and cinematic elements rather than plot details
5. Use engaging, accessible language appropriate for the target audience
6. Maintain factual accuracy about cast, year, and basic movie information

OUTPUT FORMAT:
Return a valid JSON object with the following structure:
{
  "title": "Movie Title",
  "year": "Release Year",
  "genre": "Primary Genre",
  "rating": "Rating (if available)",
  "originalSummary": "Original 150-200 word summary focusing on themes and cinematic qualities",
  "keyThemes": ["theme1", "theme2", "theme3"],
  "cinematicHighlights": ["highlight1", "highlight2"],
  "targetAudience": "Description of ideal audience",
  "emotionalTone": "Overall emotional tone/mood",
  "culturalImpact": "Brief note on cultural significance (if applicable)"
}`;
  }

  /**
   * Build prompt for movie card generation
   */
  private buildMovieCardPrompt(movie: Movie, options: MovieCardGenerationOptions): string {
    const summaryLengthMap = {
      'short': '100-150 words',
      'medium': '150-200 words',
      'long': '200-250 words'
    };

    const summaryLength = summaryLengthMap[options.summaryLength || 'medium'];
    const tone = options.tone || 'professional';
    const audience = options.targetAudience || 'general';

    return `Generate an original movie card for the following film:

MOVIE DATA:
- Title: ${movie.title}
- Year: ${movie.releaseYear || 'Unknown'}
- Director: ${movie.director || 'Unknown'}
- Plot (for reference only - DO NOT copy): ${movie.plotSummary || movie.plotShort || 'No plot available'}
- IMDB Rating: ${movie.ratingImdb || 'Not rated'}
- Runtime: ${movie.runtimeMinutes ? `${movie.runtimeMinutes} minutes` : 'Unknown'}

GENERATION REQUIREMENTS:
- Summary length: ${summaryLength}
- Tone: ${tone}
- Target audience: ${audience}
- Focus on original analysis and interpretation
- Highlight cinematic qualities, themes, and emotional impact
- Avoid copying the provided plot summary
- Create engaging, unique content that captures the film's essence

Generate a compelling, original movie card that would help users discover and appreciate this film.`;
  }

  /**
   * Parse and validate LLM response
   */
  private parseAndValidateResponse(content: string, originalMovie: Movie): LLMMovieCard {
    try {
      const parsed = JSON.parse(content);
      
      // Validate required fields
      if (!parsed.originalSummary) {
        throw new Error('Missing required fields in LLM response');
      }

      // Create LLMMovieCard object
      const movieCard: LLMMovieCard = {
        id: `llm_${originalMovie.imdbId}_${Date.now()}`,
        movieId: originalMovie.imdbId,
        generatedSummary: parsed.originalSummary,
        keyThemes: parsed.keyThemes || [],
        moodTags: parsed.emotionalTone ? [parsed.emotionalTone] : [],
        llmModel: this.config.model,
        generationTimestamp: new Date(),
        isApproved: false,
        ...(parsed.title && { generatedTitle: parsed.title }),
        ...(parsed.targetAudience && { targetAudience: parsed.targetAudience })
      };

      return movieCard;
    } catch (error) {
      throw new Error(`Failed to parse LLM response: ${error instanceof Error ? error.message : 'Invalid JSON'}`);
    }
  }

  /**
   * Generate cache key for movie card
   */
  private generateCacheKey(movie: Movie, options: MovieCardGenerationOptions): string {
    const optionsHash = JSON.stringify(options);
    return `moviecard_${movie.imdbId}_${Buffer.from(optionsHash).toString('base64')}`;
  }

  /**
   * Calculate cost based on tokens used
   */
  // private calculateCost(tokens: number): number {
  //   // GPT-4 pricing (approximate): $0.03 per 1K prompt tokens, $0.06 per 1K completion tokens
  //   // Simplified calculation assuming 50/50 split
  //   const costPer1KTokens = 0.045; // Average of prompt and completion costs
  //   return (tokens / 1000) * costPer1KTokens;
  // }

  /**
   * Delay utility for rate limiting
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Get cache statistics
   */
  getCacheStats(): { keys: number; hits: number; misses: number } {
    const stats = this.cache.getStats();
    return {
      keys: stats.keys,
      hits: stats.hits,
      misses: stats.misses
    };
  }

  /**
   * Clear cache
   */
  clearCache(): void {
    this.cache.flushAll();
    this.logger.info('LLM service cache cleared');
  }

  /**
   * Health check for LLM service
   */
  async healthCheck(): Promise<{ status: 'healthy' | 'unhealthy'; details: any }> {
    try {
      const testResponse = await this.openai.chat.completions.create({
        model: this.config.model,
        messages: [{ role: 'user', content: 'Test connection' }],
        max_tokens: 10
      });

      return {
        status: 'healthy',
        details: {
          model: this.config.model,
          cacheStats: this.getCacheStats(),
          lastResponse: testResponse.created
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        details: {
          error: error instanceof Error ? error.message : 'Unknown error',
          model: this.config.model
        }
      };
    }
  }
}